---
title: "On the Analysis of Acoustic Distance Perception in a Head Mounted Display"
date: 2017-11-01T00:00:00

# Schedule page publish date (NOT publication's date).
publishDate: '2017-11-02T00:00:00Z'

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors:
 - Felix Dollack
 - Christina Imbery
 - Joerg Bitzer

# Digital Object Identifier (DOI)
doi: "10.2312/egve.20171338"

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference paper
# 2 = Journal article
# 3 = Manuscript
# 4 = Report
# 5 = Book
# 6 = Book section
publication_types: ["1"]

# Publication name and optional abbreviated version.
publication: "In *International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments (ICAT-EGVE)*."
publication_short: "In *ICAT-EGVE*"

# Abstract.
abstract: "Recent work has shown that distance perception in virtual reality is different from reality. Several studies have tried to quantify the discrepancy between virtual and real visual distance perception but only little work was done on how visual stimuli affect acoustic distance perception in virtual environments. The present study investigates how a visual stimulus effects acoustic distance perception in virtual environments. Virtual sound sources based on binaural room impulse response (BRIR) measurements made from distances ranging from 0.9 to 4.9 m in a lecture room were used as auditory stimuli. Visual stimulation was done using a head mounted display (HMD). Participants were asked to estimate egocentric distance to the sound source in two conditions: auditory with GUI (A), auditory with HMD (A+V). Each condition was presented within its own block to a total of eight participants. We found that a systematical offset is introduced by the visual stimulus."

# Summary. An optional shortened abstract.
summary: "The present study investigates how a visual stimulus effects acoustic distance perception in virtual environments. Participants were asked to estimate egocentric distance to the sound source in two conditions: auditory with GUI (A), auditory with HMD (A+V). We found that a systematical offset is introduced by the visual stimulus."

# Is this a featured publication? (true/false)
featured: true

# Tags (optional).
#   Set `tags: []` for no tags, or use the form `tags: ["A Tag", "Another Tag"]` for one or more tags.
tags:
 - acoustic perception
 - psychoacoustics
 - VR

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ["deep-learning"]` references
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Links (optional).
url_pdf: ""
url_preprint: ""
url_code: ""
url_dataset: ""
url_project: ""
url_slides: ""
url_video: ""
url_poster: ""
url_source: ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
#links: [{name = "Custom Link", url = "http://example.org"}]

# Does this page contain LaTeX math? (true/false)
math: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  # Caption (optional)
  caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)"

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point: ""

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ''
---
