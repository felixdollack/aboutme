[{"authors":null,"categories":null,"content":"My interests lie with (animal, artificial, hybrid, human) intelligence, affective computing and human-computer interaction. I got my PhD in the field of Human Informatics at the University of Tsukuba (Japan) and received BEng and MSc degrees in Hearing Technology and Audiology from Jade University of Applied Sciences Oldenburg and the University of Oldenburg (Germany).\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://felixdollack.github.io/aboutme/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aboutme/authors/admin/","section":"authors","summary":"My interests lie with (animal, artificial, hybrid, human) intelligence, affective computing and human-computer interaction. I got my PhD in the field of Human Informatics at the University of Tsukuba (Japan) and received BEng and MSc degrees in Hearing Technology and Audiology from Jade University of Applied Sciences Oldenburg and the University of Oldenburg (Germany).","tags":null,"title":"Felix Dollack, PhD","type":"authors"},{"authors":null,"categories":null,"content":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://felixdollack.github.io/aboutme/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/aboutme/courses/example/example1/","section":"courses","summary":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://felixdollack.github.io/aboutme/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/aboutme/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://felixdollack.github.io/aboutme/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/aboutme/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["M. Perusquia-Hernandez","F. Dollack","C. K. Tan","S. Namba","S. Ayabe-Kanamura","K. Suzuki"],"categories":null,"content":"","date":1619740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619740800,"objectID":"c38073388cc421819a39e875c03f8dc6","permalink":"https://felixdollack.github.io/aboutme/publication/icmi-2020/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/aboutme/publication/icmi-2020/","section":"publication","summary":"Distal facial Electromyography (EMG) can be used to detect smiles and frowns with reasonable accuracy. It capitalizes on volume conduction to detect relevant muscle activity, even when the electrodes are not placed directly on the source muscle. The main advantage of this method is to prevent occlusion and obstruction of the facial expression production, whilst allowing EMG measurements. However, measuring EMG distally entails that the exact source of the facial movement is unknown. We propose a novel method to estimate specific Facial Action Units (AUs) from distal facial EMG and Computer Vision (CV). This method is based on Independent Component Analysis (ICA), Non-Negative Matrix Factorization (NNMF), and sorting of the resulting components to determine which is the most likely to correspond to each CV-labeled action unit (AU). Performance on the detection of AU06 (Orbicularis Oculi) and AU12 (Zygomaticus Major) was estimated by calculating the agreement with Human Coders. The results of our proposed algorithm showed an accuracy of 81% and a Cohen’s Kappa of 0.49 for AU6; and accuracy of 82% and a Cohen’s Kappa of 0.53 for AU12. This demonstrates the potential of distal EMG to detect individual facial movements. Using this multimodal method, several AU synergies were identified. We quantified the co-occurrence and timing of AU6 and AU12 in posed and spontaneous smiles using the human-coded labels, and for comparison, using the continuous CV-labels. The co-occurrence analysis was also performed on the EMG-based labels to uncover the relationship between muscle synergies and the kinematics of visible facial movement.","tags":["electromyography","computer vision","smiles","FACS","posed and spontaneous smiles"],"title":"Facial movement synergies and Action Unit detection from distal wearable Electromyography and Computer Vision","type":"publication"},{"authors":["M. Perusquía-Hernández","F. Dollack","C.-K. Tan","S. Namba","S. Ayabe-Kanamura","K.Suzuki"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"12218deedea89cc7df075299ff818d62","permalink":"https://felixdollack.github.io/aboutme/publication/sas-2021a/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/aboutme/publication/sas-2021a/","section":"publication","summary":"Distal facial Electromyography (EMG) capitalizes on volume conduction to detect relevant muscle activity, even when the electrodes are not placed directly on the muscle. We used this technique and a Sensing-Source-Synergy approach to uncover the role of the Duchenne marker in spontaneous smiles. The EMG components related to the Duchenne marker and the Lip Corner Puller appear separately during posed smile production. However, during spontaneous smiles they move synchronously.","tags":["Affective computing","spontaneous smiles","posed smiles","Duchenne marker","FACS"],"title":"Facial distal electromyography synergy analysis uncovers the relevance of the Duchenne marker in spontaneous smile production","type":"publication"},{"authors":["M. Perusquía-Hernández","F. Dollack","S. Ayabe-Kanamura","K.Suzuki"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"c9a1f2615afa57fcb5a7a0e06a9a680c","permalink":"https://felixdollack.github.io/aboutme/publication/sas-2021b/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/aboutme/publication/sas-2021b/","section":"publication","summary":"Affective processing of facial expressions is often assumed to be predominantly visual. Facial mimicry has been deemed as critical for the understanding of our own emotions and those of others. However, there is evidence that congenitally blind people display both spontaneous and posed facial expressions. This casts doubt on the role of facial mimicry in general emotion understanding. Hence, we propose to investigate smile production in the absence of vision. We devised several tasks with the goal of eliciting spontaneous, voluntary, and social smiles. Furthermore, we propose to compare the smiles of the blind to those of an average person. Pilot results showed a similar smiling behaviour. This suggests that visually mimicking the facial expressions of others is not a necessary requirement to develop spontaneous facial expression behaviour. Nevertheless, several facial involuntary movements, known as blindisms, might impair other’s recognition of the blind’s facial expressions.","tags":["Blindness","smile","production","EMG","mimicry"],"title":"Solitary and social smile production in congenital blindness","type":"publication"},{"authors":["Felix Dollack, PhD","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://felixdollack.github.io/aboutme/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/aboutme/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":[],"content":"","date":1596343504,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596343504,"objectID":"05ecab606c2bd005990ce4967e12c315","permalink":"https://felixdollack.github.io/aboutme/project/robotmirroring/","publishdate":"2020-08-02T13:45:04+09:00","relpermalink":"/aboutme/project/robotmirroring/","section":"project","summary":"Foster empathy and well-being through mirroring of human embodied states in artificial agents.","tags":["Affective Computing"],"title":"Robot Mirroring","type":"project"},{"authors":["M. Perusquia-Hernandez","D. A. Gomez Jauregui","M. Cuberos-Balda","D. F. Paez-Granados","F. Dollack","J. V. Salazar"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"84e4bd7826028c902027c6742fc5cf5f","permalink":"https://felixdollack.github.io/aboutme/publication/roman-2020/","publishdate":"2020-08-02T00:00:00Z","relpermalink":"/aboutme/publication/roman-2020/","section":"publication","summary":"Self-tracking aims to increase awareness, decrease undesired behaviors, and ultimately lead towards a healthier lifestyle. However, inappropriate communication of self-tracking results might cause the opposite effect. Subtle self-tracking feedback is an alternative that can be provided with the aid of an artificial agent representing the self. Hence, we propose a wearable pet that reflects the user's affective states through visual and haptic feedback. By eliciting empathy and fostering helping behaviors towards it, users would indirectly help themselves without explicit feedback. A wearable prototype was built, and three user studies performed to evaluate the appropriateness of the proposed affective representations. Visual representations using facial and body cues were clear for valence and less clear for arousal. Haptic interoceptive patterns emulating heart-rate levels matched the desired feedback urgency levels with a saturation frequency. The integrated visuo-haptic representations matched to participants own affective experience. From the results, we derived three design guidelines for future robot mirroring wearable systems: physical embodiment, interoceptive feedback, and customization.","tags":["affective computing","biosignals"],"title":"Robot Mirroring: Promoting Empathy with an Artificial Agent by Reflecting the User's Physiological Affective States","type":"publication"},{"authors":["Felix Dollack","Yuta Kozaki","Takeshi Ozu","Rina Katsube"],"categories":[],"content":"The original version of El Astrocade was limited due to hardware necessities. For better portability El Astrocade 2 was an attempt to minify the original game while keeping most of the control aspects the same.\nAn aluminum frame box with a acrylic display used to project the game from below was created. The final length of each side of the table was too short to make players walk. Therefore, we opted for balance control using WiiFit balance boards. Shifting weight to the left or right side controlled the movement of the spaceship. Leaning backward on the balance board triggered the shield for defense against damage.\nThe balance board has a weight limit, so we had to rethink the jump action to trigger shooting. We decided to build very soft buttons to trigger a shot. The buttons sense touch by infrared light refraction measured with custom PCB boards on the inside. Every touch was reflected with a light animation inside the button.\nThe work was exhibited during the 5-day lasting Ars Electronica Festival 2019 in Linz, Austria. It was well received and a magnet for approximately 1000 visitors a day. The total down-time during the exhibition was less than 5 minutes.\nThe source code might be released on www.github.com/felixdollack at a later point after cleaning it a little.\nProject status: finished.\n","date":1585153490,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585153490,"objectID":"c8d8ef2c2c3685df87a15eae14319764","permalink":"https://felixdollack.github.io/aboutme/project/elastrocade2/","publishdate":"2020-03-26T01:24:50+09:00","relpermalink":"/aboutme/project/elastrocade2/","section":"project","summary":"Table-sized version of El Astrocade to promote physical movement and teamwork.","tags":["Interactive Gameplay"],"title":"El Astrocade 2","type":"project"},{"authors":["F. Dollack","M. Perusquía-Hernández","H. Kadone","K. Suzuki"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"aec1cd070548c02cabbd13f9a48e2514","permalink":"https://felixdollack.github.io/aboutme/publication/mhs_dec_2019/","publishdate":"2019-12-02T00:00:00Z","relpermalink":"/aboutme/publication/mhs_dec_2019/","section":"publication","summary":"Head direction has been identified to anticipate trajectory direction during human locomotion, independently of the visual condition. However, experiments so far have explored this phenomenon with visual instructions. To explore head and gaze anticipation without visual influence, we describe a system for instruction and measurement of auditory instructed locomotion. The objective of this work is to describe the system setup and validate its accuracy for scientific investigations in locomotion research. The system is comprised of an auralization server that plays virtual sound sources via wireless headphones. The auralization is fed with head position and orientation angle measured by a Vicon motion tracking system. Communication between the auralization server and the motion tracking host was facilitating the OSC protocol. Auditory instructed locomotion was performed in two visual condition: eyes open and eyes closed for comparison. First, ten sighted participants localized static virtual sound sources at eight positions to confirm the accuracy of the provided virtual sound cues. Afterwards, they listened and actively followed virtual sound sources that were moving along three trajectories: an eight shape, a circle in clockwise direction and a circle in anticlockwise direction. The virtual sound sources could be localized with an accuracy of 0.16 meters in the eyes open condition and 0.12 meters in the eyes closed condition. Participants also were able to follow the moving sound sources.","tags":["virtual sound","localization","locomotion"],"title":"Auditory Locomotion Guidance System For Spatial Localization","type":"publication"},{"authors":["F. Dollack","H. Kadone","M. Perusquía-Hernández","K. Suzuki"],"categories":null,"content":"","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561852800,"objectID":"aac9690120e1c78b169cd932b1b7f31c","permalink":"https://felixdollack.github.io/aboutme/publication/ispgr-june-2019-late/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/aboutme/publication/ispgr-june-2019-late/","section":"publication","summary":"","tags":["motor planning","gait","eye movement"],"title":"Head anticipation during auditory instructed locomotion​","type":"publication"},{"authors":["F. Dollack","M. Perusquía-Hernández","H. Kadone","K. Suzuki"],"categories":null,"content":"","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"fbe2bb40d6530bc4b781b4a766a2a192","permalink":"https://felixdollack.github.io/aboutme/publication/ispgr-june-2019/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/aboutme/publication/ispgr-june-2019/","section":"publication","summary":"","tags":["motor planning","gait","eye movement"],"title":"Effect of voluntary gaze movement on gait steering control","type":"publication"},{"authors":["F. Dollack","M. Perusquía-Hernández","H. Kadone","K. Suzuki"],"categories":null,"content":"","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"003c7b9d8ce5fe0a58f1fd8848c5699d","permalink":"https://felixdollack.github.io/aboutme/publication/frontiers-aug-2019/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/aboutme/publication/frontiers-aug-2019/","section":"publication","summary":"Head direction has been identified to anticipate trajectory direction during human locomotion. Head anticipation has also been shown to persist in darkness. Arguably, the purpose for this anticipatory behavior is related to motor control and trajectory planning, independently of the visual condition. This implies that anticipation remains in the absence of visual input. However, experiments so far have only explored this phenomenon with visual instructions which intrinsically primes a visual representation to follow. The primary objective of this study is to describe head anticipation in auditory instructed locomotion, in the presence and absence of visual input. Auditory instructed locomotion trajectories were performed in two visual conditions: eyes open and eyes closed. First, 10 sighted participants localized static sound sources to ensure they could understand the sound cues provided. Afterwards, they listened to a moving sound source while actively following it. Later, participants were asked to reproduce the trajectory of the moving sound source without sound. Anticipatory head behavior was observed during trajectory reproduction in both eyes open and closed conditions. The results suggest that head anticipation is related to motor anticipation rather than mental simulation of the trajectory.","tags":["acoustic perception","psychoacoustics","head anticipation","gait"],"title":"Gaze and head anticipation during locomotion with auditory instruction in the presence and absence of visual input","type":"publication"},{"authors":["Felix Dollack","Yuta Kozaki","Takeshi Ozu","Rina Katsube","Minatsu Sugimoto"],"categories":null,"content":"We, the authors, proposed a shooting game based on whole body motion as part of a student project. Over the course of 12 months we ran through multiple iterations of designing the appearance and interactions within the game. The core elements of the finalized version are motion tracking and a helmet with haptic and visual feedback.\nFirst iteration of the game projection: The helmet (picture below) is used to present haptic feedback when the spaceships receive damage through lasers or by collision with asteroids. When the ship gets hit a LED strip below the visor flashes red. The rest of the time the LED strip fades from green to red to indicate the life points of the spaceship. A display at the front of the helmet shows the team color. Motion tracking markers on top of the hemet (not in the picture) are used to track the position of the players.\nThe body position (x,y,z) of 2 teams with 2 players each is measured to give shared control over a spaceship to each team. One player of each team controls the X and Y direction of the ship. The players height controls the ships actions. At the beginning of each game the height of the players is picked up. The ship can shoot a laser when either of the team members jumps or activate a shield for defense when one player of the team squats during the game.\nThis game is entirely written in C++ with openFrameworks and was developed for the Large Space at the University of Tsukuba. The game graphics are projected on a 5 by 5 meter floor area and players move along the sides of the projection.\nThis final system increases physical activity of all players thanks to the body motion control and fosters teamwork through shared control of the spaceship.\nThe source code might be released on www.github.com/felixdollack at a later point after cleaning it a little.\nProject status: finished.\n","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"58f6d76b2847af6316200cf6888387ad","permalink":"https://felixdollack.github.io/aboutme/project/elastrocade/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/aboutme/project/elastrocade/","section":"project","summary":"A large floor projection game to promote physical movement and teamwork.","tags":["Floor projection","Interactive Gameplay"],"title":"El Astrocade","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://felixdollack.github.io/aboutme/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/aboutme/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["F. Dollack","C. Imbery","J. Bitzer"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"995ebf1a3b2abb05b15b3a05315a27b0","permalink":"https://felixdollack.github.io/aboutme/publication/icat-nov-2017/","publishdate":"2017-11-02T00:00:00Z","relpermalink":"/aboutme/publication/icat-nov-2017/","section":"publication","summary":"The present study investigates how a visual stimulus effects acoustic distance perception in virtual environments. Participants were asked to estimate egocentric distance to the sound source in two conditions: auditory with GUI (A), auditory with HMD (A+V). We found that a systematical offset is introduced by the visual stimulus.","tags":["acoustic perception","psychoacoustics","VR"],"title":"On the Analysis of Acoustic Distance Perception in a Head Mounted Display","type":"publication"},{"authors":["M. Hansen","F. Dollack","S. Raufer","H.-L. Grahlmann","G. Eberlei"],"categories":null,"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"8704ceedcd90e4fdff3a79e26f03541e","permalink":"https://felixdollack.github.io/aboutme/publication/daga-march-2013/","publishdate":"2013-03-02T00:00:00Z","relpermalink":"/aboutme/publication/daga-march-2013/","section":"publication","summary":"","tags":["acoustic perception","psychoacoustics","speech intelligibility"],"title":"Speech intelligibility in realistic listening situations for different numbers, azimuths and movement of speech or noise maskers","type":"publication"}]